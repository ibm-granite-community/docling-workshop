{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nshGZ8GVNe9v"
   },
   "source": [
    "# Enhanced Chunking and Vectorization with Docling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jab6mCqrNgDU"
   },
   "source": [
    "## Lab 2: Advanced Document Processing\n",
    "\n",
    "In our previous workshop, you learned how to convert documents with Docling. Now, we'll tackle the next critical challenge: **how to intelligently chunk those documents for optimal retrieval in AI systems**. This is where many RAG implementations succeed or fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q53hy7sRNhv_"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- **Understand** why chunking is critical for RAG system performance\n",
    "- **Compare** different chunking strategies and their trade-offs\n",
    "- **Implement** hierarchical and hybrid chunking with Docling\n",
    "- **Evaluate** chunk quality using practical metrics\n",
    "- **Choose** the right chunking approach for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKYHSl7tNjjN"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.10, 3.11, or 3.12\n",
    "- Basic understanding of RAG (Retrieval Augmented Generation) concepts\n",
    "- Familiarity with text processing and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70r3W375NoEZ"
   },
   "source": [
    "## What You'll Build\n",
    "\n",
    "We'll create a complete document chunking pipeline that:\n",
    "1. Processes PDF documents using Docling's advanced capabilities\n",
    "2. Applies multiple chunking strategies with clear comparisons\n",
    "3. Visualizes chunk characteristics for quality assessment\n",
    "4. Provides actionable guidance for strategy selection\n",
    "5. Prepares chunks for embedding and vector storage\n",
    "\n",
    "More importantly, you'll gain the intuition to make informed chunking decisions for your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjnLwuCKN08G"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgaslaURN1tf"
   },
   "source": [
    "## Understanding Retrieval Augmented Generation (RAG) and Chunking's Critical Role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWPbtJGrdKl6"
   },
   "source": [
    "### What is RAG and Why Does It Matter?\n",
    "\n",
    "[Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) has emerged as the dominant pattern for building AI applications that need access to private or recent data. Unlike pure language models that rely solely on training data, RAG systems dynamically retrieve relevant information to generate accurate, grounded responses.\n",
    "\n",
    "Think of RAG as giving an AI assistant access to a perfectly organized library. The quality of the organization (chunking) directly determines how well the assistant can find and use information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM3yXSVHdlHe"
   },
   "source": [
    "### The RAG Pipeline: Where Chunking Fits\n",
    "\n",
    "The complete RAG process involves six interconnected steps:\n",
    "\n",
    "1. **Document Processing**: Converting raw documents (PDFs, Word, etc.) into structured formats\n",
    "2. **Chunking**: Breaking documents into semantically meaningful pieces ← **Today's focus**\n",
    "3. **Vectorization**: Converting text chunks into numerical embeddings\n",
    "4. **Indexing**: Storing vectors in a specialized database for similarity search\n",
    "5. **Retrieval**: Finding the most relevant chunks for a given query\n",
    "6. **Generation**: Using retrieved context to generate accurate, grounded responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBAbXNCDdtyd"
   },
   "source": [
    "### Why is Chunking Critical?\n",
    "\n",
    "Chunking is arguably the most important yet underappreciated step in the RAG pipeline. Here's why:\n",
    "\n",
    "#### 1. **Context Window Constraints**\n",
    "Modern embedding models have strict token limits:\n",
    "- IBM Granite embeddings: 512 tokens\n",
    "- Cohere embed-v3: 512 tokens\n",
    "- OpenAI's text-embedding-3-small: 8,191 tokens\n",
    "\n",
    "Chunks must fit within these limits while preserving meaning.\n",
    "\n",
    "#### 2. **Retrieval Precision vs. Recall Trade-off**\n",
    "- **Small chunks**: High precision (exact matches) but may miss context\n",
    "- **Large chunks**: Better context but lower precision (irrelevant content)\n",
    "\n",
    "Finding the sweet spot is crucial for system performance.\n",
    "\n",
    "#### 3. **Semantic Coherence**\n",
    "Well-designed chunks maintain topical unity. A chunk about \"quarterly revenue\" shouldn't suddenly switch to \"employee benefits\" mid-way through.\n",
    "\n",
    "#### 4. **Cost and Performance**\n",
    "- Smaller chunks = More embeddings = Higher costs\n",
    "- Larger chunks = Fewer embeddings but potentially worse retrieval\n",
    "\n",
    "#### 5. **User Experience**\n",
    "The quality of retrieved chunks directly impacts the relevance and accuracy of generated responses. Poor chunking leads to hallucinations, missing information, or irrelevant answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNj6oJ9ceDOz"
   },
   "source": [
    "### The Chunking Challenge: A Practical Example\n",
    "\n",
    "Let's illustrate why naive chunking fails and intelligent chunking succeeds:\n",
    "\n",
    "**Bad Chunking Example (Fixed-size splitting):**\n",
    "```\n",
    "Chunk 1: \"The company's revenue increased by 25% in Q3\"\n",
    "Chunk 2: \"2024 compared to Q3 2023. This growth was driven by...\"\n",
    "```\n",
    "**Problems**:\n",
    "- Critical context (which year?) is split across chunks\n",
    "- A search for \"2024 revenue growth\" might miss Chunk 1 entirely\n",
    "- The model lacks complete information to answer accurately\n",
    "\n",
    "**Good Chunking Example (Semantic-aware):**\n",
    "```\n",
    "Chunk 1: \"Financial Performance Q3 2024: The company's revenue increased by\n",
    "          25% in Q3 2024 compared to Q3 2023, reaching $1.2B in total sales.\"\n",
    "          \n",
    "Chunk 2: \"Growth Drivers: This exceptional growth was driven by strong\n",
    "          performance in the enterprise segment, with cloud services\n",
    "          contributing 60% of the increase...\"\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Complete, self-contained thoughts\n",
    "- Clear topical boundaries\n",
    "- Sufficient context for accurate retrieval\n",
    "- Natural section breaks preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z4EG67feVhd"
   },
   "source": [
    "### What Makes a Good Chunk?\n",
    "\n",
    "Before we dive into implementation, let's establish the characteristics of effective chunks:\n",
    "\n",
    "1. **Self-contained**: Can be understood without external context\n",
    "2. **Topically coherent**: Focuses on a single topic or concept\n",
    "3. **Appropriately sized**: Fits within model limits with headroom\n",
    "4. **Contextually rich**: Includes necessary metadata (source, section, page)\n",
    "5. **Semantically complete**: Doesn't cut off mid-sentence or mid-thought\n",
    "\n",
    "Now, let's use Docling's capabilities build a system that creates these ideal chunks automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT5y_QW8ebsn"
   },
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Let's begin by setting up our environment with all necessary dependencies. We'll install not just Docling, but also the tools needed for chunking, vectorization, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEoM938B_JRH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BfMWUUSs_JRI",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2a03c19a-a72a-44dd-c2ba-64e6c25a8681",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    pillow \\\n",
    "    langchain_community \\\n",
    "    'langchain_huggingface[full]' \\\n",
    "    langchain_milvus \\\n",
    "    docling \\\n",
    "    matplotlib \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKnuBqIBekTi"
   },
   "source": [
    "Now let's import the essential modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZxWTGAFNIxK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any, Iterator, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inXV8p7EIvag"
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0jUQOfNIvag"
   },
   "source": [
    "To see detailed information about the document processing and chunking operations, we'll configure INFO log level.\n",
    "\n",
    "NOTE: It is okay to skip running this cell if you prefer less verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQV4OW29Ivag"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D49OmkCJKEEc"
   },
   "source": [
    "## Document Processing with Docling\n",
    "\n",
    "### Understanding Document Structure\n",
    "\n",
    "Before we can chunk intelligently, we need to understand how Docling preserves document structure. This structural awareness is what separates naive text splitting from semantic chunking.\n",
    "\n",
    "Let's start by processing a real document and exploring its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xow-PjeOKJA2",
    "outputId": "f2e8dfe7-3f17-4163-bfcc-3d69b42fcac0"
   },
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "# Configure pipeline to extract both text and images\n",
    "pdf_pipeline_options = PdfPipelineOptions(\n",
    "    do_ocr=False,   # Skip OCR for faster processing (enable for scanned docs)\n",
    "    generate_picture_images=True,   # Extract images for multi-modal applications\n",
    ")\n",
    "format_options = {\n",
    "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
    "}\n",
    "converter = DocumentConverter(format_options=format_options)\n",
    "\n",
    "# Process a sample document\n",
    "sample_doc_url = \"https://midwestfoodbank.org/images/AR_2020_WEB2.pdf\"\n",
    "result = converter.convert(sample_doc_url)\n",
    "doc = result.document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRRVfbMWiLKy"
   },
   "source": [
    "**Why this document?**\n",
    "We're using an annual report because it contains:\n",
    "- Hierarchical sections (perfect for testing structural chunking)\n",
    "- Mixed content types (text, tables, images)\n",
    "- Varying paragraph lengths (tests size-aware chunking)\n",
    "- Real-world complexity (not a toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGKctBMFiSIU"
   },
   "source": [
    "### The Importance of Structure Preservation\n",
    "\n",
    "Docling doesn't just extract text; it preserves the document's logical structure. This includes:\n",
    "- **Headings and subheadings**: Natural chunk boundaries\n",
    "- **Paragraphs**: Semantic units of thought\n",
    "- **Lists and tables**: Special handling required\n",
    "- **Captions and references**: Important context\n",
    "\n",
    "This structural information is very important for intelligent chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mP_mj4dBiWFW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dmNeNMIInSH"
   },
   "source": [
    "## Chunk Visualization: See What You're Building\n",
    "\n",
    "Before we dive into chunking strategies, let's build a powerful visualization tool. Being able to \"see\" your chunks is invaluable for understanding and debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY1x4I-nibza"
   },
   "source": [
    "### Building a Comprehensive Chunk Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewCZQfo0IteY"
   },
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker.tokenizer.base import BaseTokenizer\n",
    "from docling.chunking import BaseChunker, BaseChunk\n",
    "from docling.datamodel.document import DoclingDocument\n",
    "\n",
    "def visualize_chunks(chunks: list[BaseChunk], *, chunker: BaseChunker, tokenizer: BaseTokenizer, title=\"Document Chunks\"):\n",
    "    \"\"\"Visualize chunk sizes and distribution in tokens.\n",
    "    \"\"\"\n",
    "    # Extract token counts for each chunk\n",
    "    token_counts = [tokenizer.count_tokens(chunker.contextualize(chunk=chunk)) for chunk in chunks]\n",
    "\n",
    "    # Create histogram with all annotations in one go\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the histogram\n",
    "    plt.hist(token_counts, bins=20, alpha=0.7, color='skyblue')\n",
    "\n",
    "    # Add statistics line and annotations\n",
    "    avg_tokens = np.mean(token_counts)\n",
    "    plt.axvline(avg_tokens, color='red', linestyle='--', label=f'Average: {avg_tokens:.1f}')\n",
    "\n",
    "    # Add labels and formatting\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Chunk Size (tokens)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the complete plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print comprehensive statistics\n",
    "    print(f\"Chunk Analysis Results:\")\n",
    "    print(f\"Total chunks: {len(token_counts)}\")\n",
    "    print(f\"Average chunk size: {np.mean(token_counts):.1f} tokens\")\n",
    "    print(f\"Minimum chunk size: {min(token_counts)} tokens\")\n",
    "    print(f\"Maximum chunk size: {max(token_counts)} tokens\")\n",
    "    print(f\"Standard deviation: {np.std(token_counts):.1f} tokens\")\n",
    "\n",
    "    # Quality indicators\n",
    "    if max(token_counts) > 512:\n",
    "        print(\"Warning: Some chunks exceed 512 tokens - consider reducing chunk size\")\n",
    "    if np.std(token_counts) > 100:\n",
    "        print(\"Warning: High variance in chunk sizes - retrieval consistency may suffer\")\n",
    "\n",
    "    # Also show character length for reference\n",
    "    char_lengths = [len(chunk.text) for chunk in chunks]\n",
    "    print(f\"\\nReference - Average character length: {np.mean(char_lengths):.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEh8rIP-I9mO"
   },
   "source": [
    "## Docling Chunking Fundamentals: Understanding the Architecture\n",
    "\n",
    "Before we explore specific strategies, let's understand Docling's chunking architecture. This knowledge will help you extend or customize chunking for your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDNwHE0vJKi7"
   },
   "source": [
    "### The BaseChunker Interface\n",
    "\n",
    "Docling defines a BaseChunker interface that all chunkers must implement. This interface includes two main methods:\n",
    "\n",
    "1. `chunk(dl_doc)`: Returns an iterator of chunks for the provided document\n",
    "2. `serialize(chunk)`: Returns the serialized representation of a chunk, typically used for embedding\n",
    "\n",
    "Let's examine this interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymJMe-hLJKEQ"
   },
   "outputs": [],
   "source": [
    "# Example of BaseChunker interface structure\n",
    "class SimpleChunker(BaseChunker):\n",
    "    \"\"\"A simple example chunker implementing the BaseChunker interface.\"\"\"\n",
    "\n",
    "    def chunk(self, dl_doc: DoclingDocument, **kwargs) -> Iterator[BaseChunk]:\n",
    "        \"\"\"Return chunks for the provided document.\"\"\"\n",
    "        # Simple implementation: one chunk per page\n",
    "        for i, page in enumerate(dl_doc.pages):\n",
    "            text = \" \".join([item.text for item in page.items if hasattr(item, \"text\")])\n",
    "            metadata = {\n",
    "                \"page\": i,\n",
    "                \"source\": dl_doc.name\n",
    "            }\n",
    "            yield BaseChunk(text=text, metadata=metadata)\n",
    "\n",
    "    def serialize(self, chunk: BaseChunk) -> str:\n",
    "        \"\"\"Serialize a chunk for embedding.\"\"\"\n",
    "        # Simple serialization: just return the text\n",
    "        return chunk.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqGb2G5gjAxG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZyabMjFysjd"
   },
   "source": [
    "## Chunking Strategies Deep Dive\n",
    "\n",
    "Now let's explore Docling's built-in chunking strategies. Each has its strengths and ideal use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wmv0yPhEJSyC"
   },
   "source": [
    "### Strategy 1: HierarchicalChunker - Respecting Document Structure\n",
    "\n",
    "The HierarchicalChunker preserves the natural organization of documents by following sections, subsections, and paragraphs. This is ideal for structured documents like reports, manuals, and academic papers.\n",
    "\n",
    "**When to use HierarchicalChunker:**\n",
    "- Structured documents (reports, manuals, academic papers)\n",
    "- When preserving document hierarchy is important\n",
    "- Documents with clear section breaks\n",
    "\n",
    "Avoid for: Chat logs, social media posts, unstructured text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrDVxflTPG34"
   },
   "outputs": [],
   "source": [
    "# Import the tokenizer for HybridChunker\n",
    "from transformers import AutoTokenizer\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "\n",
    "# Set up the tokenizer - using IBM Granite for this example\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(embeddings_model_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "iXvpL4g3JSSq",
    "outputId": "683fe8a3-c887-46e2-e122-122570e6bb62"
   },
   "outputs": [],
   "source": [
    "# Create a HierarchicalChunker\n",
    "hierarchical_chunker = HierarchicalChunker()\n",
    "\n",
    "# Generate chunks\n",
    "hierarchical_chunks = list(hierarchical_chunker.chunk(doc))\n",
    "\n",
    "# Visualize the chunks\n",
    "print(f\"Generated {len(hierarchical_chunks)} chunks with HierarchicalChunker\")\n",
    "visualize_chunks(\n",
    "    chunks=hierarchical_chunks,\n",
    "    title=\"HierarchicalChunker Chunks\",\n",
    "    chunker=hierarchical_chunker,\n",
    "    tokenizer=embeddings_tokenizer,\n",
    ")\n",
    "\n",
    "# Examine chunk structure\n",
    "sample_chunk = hierarchical_chunks[2]\n",
    "print(f\"\\nSample Chunk Analysis:\")\n",
    "print(f\"Text (first 200 chars): {sample_chunk.text[:200]}...\")\n",
    "print(f\"Chunk type: {type(sample_chunk).__name__}\")\n",
    "\n",
    "# Print available metadata\n",
    "if hasattr(sample_chunk, 'dl_meta'):\n",
    "    print(\"Document metadata available in 'dl_meta'\")\n",
    "elif hasattr(sample_chunk, 'meta'):\n",
    "    print(f\"Document metadata available in 'meta'\")\n",
    "    print(f\"Meta preview: {str(sample_chunk.meta)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6NhNEGZMmjE"
   },
   "source": [
    "### Strategy 2: HybridChunker - Balancing Structure and Size\n",
    "\n",
    "The HybridChunker combines the best of both worlds: it starts with hierarchical chunking but then applies size-aware splitting and merging. This ensures chunks are neither too large nor too small.\n",
    "\n",
    "**When to use HybridChunker:**\n",
    "- Most production RAG systems (recommended default)\n",
    "- When you need consistent chunk sizes\n",
    "- Mixed document types in the same system\n",
    "- When embedding model has strict token limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "HUkuIu58Motk",
    "outputId": "ca8c5db6-e7a5-4c78-faaa-136a09cf7e6c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import HybridChunker\n",
    "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
    "\n",
    "# Create a HybridChunker with default settings\n",
    "hybrid_chunker = HybridChunker(\n",
    "    tokenizer=embeddings_tokenizer,\n",
    ")\n",
    "\n",
    "# Generate chunks\n",
    "hybrid_chunks = list(hybrid_chunker.chunk(doc))\n",
    "\n",
    "# Analyze the results\n",
    "print(f\"HybridChunker Results:\")\n",
    "print(f\"Generated {len(hybrid_chunks)} chunks\")\n",
    "\n",
    "visualize_chunks(\n",
    "    chunks=hybrid_chunks,\n",
    "    title=\"HybridChunker: Structure + Size Aware\",\n",
    "    chunker=hybrid_chunker,\n",
    "    tokenizer=embeddings_tokenizer,\n",
    ")\n",
    "\n",
    "# Compare with HierarchicalChunker\n",
    "print(f\"\\nStrategy Comparison:\")\n",
    "print(f\"HierarchicalChunker: {len(hierarchical_chunks)} chunks\")\n",
    "print(f\"HybridChunker: {len(hybrid_chunks)} chunks\")\n",
    "print(f\"Reduction: {((len(hierarchical_chunks) - len(hybrid_chunks)) / len(hierarchical_chunks) * 100):.1f}%\")\n",
    "\n",
    "# Examine a sample chunk with context\n",
    "sample_hybrid_chunk = hybrid_chunks[0]\n",
    "print(f\"\\nSample HybridChunker Chunk:\")\n",
    "print(f\"Text (first 200 chars): {sample_hybrid_chunk.text[:200]}...\")\n",
    "\n",
    "if hasattr(sample_hybrid_chunk, 'meta'):\n",
    "    print(f\"Metadata available - includes structural information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpivbK9wRt6Y"
   },
   "source": [
    "## Advanced Configuration and Fine-Tuning\n",
    "\n",
    "Now let's explore advanced configurations to optimize chunking for specific needs. The key is understanding how different parameters affect your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry0lZ7UikxrX"
   },
   "source": [
    "### Understanding the Impact of Chunk Size\n",
    "\n",
    "Different chunk sizes serve different purposes. Let's explore how to configure HybridChunker for specific needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "id": "M7oAyRDMRuvG",
    "outputId": "9e4b8f96-444f-48f1-ede0-5887f881c5a5"
   },
   "outputs": [],
   "source": [
    "# Create a more constrained tokenizer for demonstration\n",
    "\n",
    "max_tokens=128  # Smaller chunks for fine-grained retrieval\n",
    "\n",
    "adv_tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(embeddings_model_path),\n",
    "    max_tokens=max_tokens,\n",
    ")\n",
    "\n",
    "adv_chunker = HybridChunker(\n",
    "    tokenizer=adv_tokenizer,\n",
    ")\n",
    "\n",
    "adv_chunks = list(adv_chunker.chunk(doc))\n",
    "\n",
    "print(f\"Advanced HybridChunker Results (64 token limit):\")\n",
    "print(f\"Generated {len(adv_chunks)} chunks\")\n",
    "\n",
    "visualize_chunks(\n",
    "    chunks=adv_chunks,\n",
    "    title=f\"HybridChunker with {max_tokens} Token Limit\",\n",
    "    chunker=adv_chunker,\n",
    "    tokenizer=adv_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcUsjKUunl6Q"
   },
   "source": [
    "### The Power of Contextualization\n",
    "\n",
    "Another one of Docling's advanced features is contextualization - adding relevant surrounding information to chunks for better retrieval. Let's see it in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDjWAiSHns_A",
    "outputId": "98e42fe5-3c37-4adf-cbd1-84c38c83e837"
   },
   "outputs": [],
   "source": [
    "# Demonstrate contextualization - how chunks get additional context\n",
    "print(f\"\\n Understanding Contextualization:\")\n",
    "print(f\"Contextualization adds relevant surrounding information to improve retrieval quality.\\n\")\n",
    "\n",
    "for i, chunk in enumerate(adv_chunks[:5]):\n",
    "    tokens_text = adv_tokenizer.count_tokens(chunk.text)\n",
    "    contextualized = adv_chunker.contextualize(chunk)\n",
    "    tokens_contextualized = adv_tokenizer.count_tokens(contextualized)\n",
    "\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"Original text ({tokens_text} tokens): {chunk.text[:100]}...\")\n",
    "    print(f\"Contextualized ({tokens_contextualized} tokens): {contextualized[:100]}...\")\n",
    "    print(f\"Context added: {tokens_contextualized - tokens_text} tokens\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr4AxT0orSwv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhElk4IcoAIF"
   },
   "source": [
    "### Choosing the Right Configuration\n",
    "\n",
    "Here's a decision framework for chunk size selection:\n",
    "\n",
    "#### Small Chunks (50-150 tokens)\n",
    "**Use when:**\n",
    "- You need precise fact retrieval\n",
    "- Documents contain many discrete facts\n",
    "- Storage cost is not a concern\n",
    "- You have good reranking capabilities\n",
    "\n",
    "**Example use cases:**\n",
    "- FAQ systems\n",
    "- Definition lookups\n",
    "- Fact-checking applications\n",
    "\n",
    "#### Medium Chunks (150-300 tokens)\n",
    "**Use when:**\n",
    "- Balancing precision and context\n",
    "- Simple RAG systems\n",
    "- Mixed query types\n",
    "- Standard embedding models\n",
    "\n",
    "**Example use cases:**\n",
    "- Customer support systems\n",
    "- Documentation search\n",
    "- Knowledge management\n",
    "\n",
    "#### Large Chunks (300-512+ tokens)\n",
    "**Use when:**\n",
    "- Context is crucial\n",
    "- Narrative flow matters\n",
    "- Fewer chunks preferred\n",
    "- Cost optimization needed\n",
    "\n",
    "**Example use cases:**\n",
    "- Legal document analysis\n",
    "- Research paper comprehension\n",
    "- Story understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Php_uSXfoFYo"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcjWtIFZ368O"
   },
   "source": [
    "## Next Steps and Advanced Topics\n",
    "\n",
    "Congratulations! You now understand the fundamentals of document chunking with Docling. Here's how to continue your journey:\n",
    "\n",
    "### Next Steps:\n",
    "1. **Try with your own documents**: Test different strategies on your specific content\n",
    "2. **Optimize parameters**: Fine-tune chunk sizes based on your embedding model\n",
    "3. **Integrate with vector databases**: Connect your chunking pipeline to Milvus, Pinecone, or similar\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "- **Custom chunking strategies**: Implement domain-specific chunking rules\n",
    "- **Multi-modal chunking**: Handle documents with images and tables\n",
    "- **Streaming chunking**: Process large document collections efficiently\n",
    "- **Chunk overlap optimization**: Find the sweet spot for your retrieval quality\n",
    "- **Semantic chunking**: Use embeddings to create semantically coherent chunks\n",
    "\n",
    "### Resources for Further Learning:\n",
    "- [Docling Documentation](https://docling-project.github.io/docling/)\n",
    "- [RAG Best Practices Guide](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [Chunking Strategies Research](https://arxiv.org/search/?query=text+chunking+retrieval)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
