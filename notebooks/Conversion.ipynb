{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk7eDm0UvuV4"
   },
   "source": [
    "# Transform Your Documents into AI-Ready Data with Docling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB108vNzvvh7"
   },
   "source": [
    "## Lab 1: Document Conversion\n",
    "\n",
    "Welcome to the first lab in our comprehensive Docling workshop series! This three-part journey will take you from document processing basics to building cutting-edge, transparent AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCZLuxodwI8T"
   },
   "source": [
    "## Overview: What You'll Build\n",
    "\n",
    "In this first lab, you'll learn how to transform complex documents into AI-ready structured data. But this isn't just about extraction - it's about intelligent conversion that preserves everything important for downstream AI applications.\n",
    "\n",
    "### By the End of This Lab\n",
    "\n",
    "You'll have mastered:\n",
    "- **Document Loading**: Ingesting PDFs, Word docs, PowerPoints, and more\n",
    "- **Structure Extraction**: Preserving hierarchy and relationships\n",
    "- **Table Excellence**: Converting complex tables into usable formats\n",
    "- **Image Handling**: Extracting and preparing images for AI processing\n",
    "- **Metadata Preservation**: Maintaining information needed for visual grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIDChMLKwPc6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKOJX7mz8kwr"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**[Docling](https://docling-project.github.io/docling/)** is an open-source toolkit for document processing, parsing, and conversion designed for generative AI applications.\n",
    "\n",
    "### Key Features\n",
    "- **Multi-format support**: PDF, DOCX, XLSX, HTML, images, and more\n",
    "- **Advanced PDF understanding**: Page layout, reading order, table structure, code blocks, formulas\n",
    "- **Unified DoclingDocument representation**: Consistent data structure across all formats\n",
    "- **Flexible export options**: Markdown, HTML, JSON, DocTags\n",
    "- **Local execution**: Process sensitive data without external services\n",
    "- **Framework integration**: LangChain, LlamaIndex, and other AI frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luYAnGcJwlzr"
   },
   "source": [
    "## Why we use Docling for Document Conversion\n",
    "\n",
    "Data is the foundation for all AI systems. In order to leverage as much data as possible, we need to be able to ingest data of various formats with accuracy. Howver, LLM's typically requre data in a specific format, thus the need for conversion.\n",
    "\n",
    "**Without proper conversion**:\n",
    "- Information gets lost or jumbled\n",
    "- Tables become unreadable text\n",
    "- Images disappear entirely\n",
    "- Document structure is destroyed\n",
    "\n",
    "**With Docling's advanced conversion**:\n",
    "- Every piece of information is preserved\n",
    "- Tables maintain their structure\n",
    "- Images are extracted and processable\n",
    "- Layout and relationships are understood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgcU9jeR8xLd"
   },
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "Ensure you are running Python 3.10, 3.11 or 3.12 in a freshly created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "U4UTqpiL8Dfe"
   },
   "outputs": [],
   "source": [
    "! uv pip install \"docling[vlm]\" matplotlib pillow pandas ipywidgets python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV-71yke857S"
   },
   "source": [
    "### Import Essential Components\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eg9Lln_89Cv"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Core Docling imports\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import PdfFormatOption\n",
    "\n",
    "# For advanced features\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem, TextItem, DoclingDocument\n",
    "\n",
    "# For data processing and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N0cmabF_Ria"
   },
   "source": [
    "## Basic Document Conversion\n",
    "\n",
    "### Minimal Example\n",
    "\n",
    "The simplest way to convert a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjeZWdaMQq3j"
   },
   "outputs": [],
   "source": [
    "# example document: Docling Technical Report\n",
    "docling_paper = \"https://arxiv.org/pdf/2501.17887\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRBIgujl_01F"
   },
   "outputs": [],
   "source": [
    "# Simple conversion\n",
    "\n",
    "# Create a converter instance\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Convert a document\n",
    "result = converter.convert(docling_paper)\n",
    "doc = result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Markdown\n",
    "md_out = doc.export_to_markdown()\n",
    "\n",
    "# printing an excerpt\n",
    "print(f\"{md_out[:2000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxDEG2s-lSLq"
   },
   "source": [
    "### Document Structure Exploration\n",
    "\n",
    "One of Docling's superpowers is understanding document structure. This becomes critical in Lab 2 (chunking) and Lab 3 (visual grounding).\n",
    "\n",
    "Let's explore what Docling discovers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJv614U1lS1x"
   },
   "outputs": [],
   "source": [
    "# Document metadata - important for tracking\n",
    "print(f\"Document title: {doc.name}\")\n",
    "print(f\"Number of pages: {len(doc.pages)}\")\n",
    "print(f\"Number of tables: {len(doc.tables)}\")\n",
    "print(f\"Number of pictures: {len(doc.pictures)}\")\n",
    "\n",
    "# Explore the document structure\n",
    "print(\"\\nDocument structure:\")\n",
    "for i, (item, level) in enumerate(doc.iterate_items()):\n",
    "    if i < 10:  # Show first 10 items\n",
    "        item_type = type(item).__name__\n",
    "        text_preview = item.text[:200] if hasattr(item, 'text') else 'N/A'\n",
    "        print(f\"{'  ' * level}{item_type}: {text_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrrwFDxmHsbz"
   },
   "source": [
    "## Export Formats and Options\n",
    "\n",
    "Docling supports multiple export formats with various options:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiCbsd1ifnns"
   },
   "outputs": [],
   "source": [
    "# Export to different formats (various options available, but called with default ones)\n",
    "markdown_text = doc.export_to_markdown()\n",
    "html_text = doc.export_to_html()\n",
    "json_dict = doc.export_to_dict()\n",
    "doc_tags = doc.export_to_doctags()\n",
    "\n",
    "# Save different formats (various options available, some shown)\n",
    "doc.save_as_markdown(\n",
    "    output_dir / \"document.md\",\n",
    "    image_mode=ImageRefMode.PLACEHOLDER,\n",
    "    image_placeholder=\"<!-- my image placeholder -->\",\n",
    "    # ...\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "# Export as JSON\n",
    "doc.save_as_json(\n",
    "    output_dir / \"document.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HoHUwTRlwMF"
   },
   "source": [
    "## Working with Tables\n",
    "\n",
    "Docling provides excellent table extraction capabilities, for this example, let's use a document with more tables, found [here](https://midwestfoodbank.org/images/AR_2020_WEB2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qyW1roXzD_E"
   },
   "source": [
    "### Basic Table Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example document with tables\n",
    "tables_example = \"https://arxiv.org/pdf/2502.09927\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iSBDPf_f4gb"
   },
   "outputs": [],
   "source": [
    "# Convert a document with tables\n",
    "table_result = converter.convert(tables_example)\n",
    "table_doc = table_result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDocument contains {len(table_doc.tables)} tables\")\n",
    "\n",
    "# Export all tables\n",
    "for table_idx, table in enumerate(table_doc.tables):\n",
    "    # Convert to pandas DataFrame\n",
    "    df = table.export_to_dataframe()\n",
    "\n",
    "    print(f\"\\n## Table {table_idx}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Save as CSV\n",
    "    df.to_csv(output_dir / f\"table_{table_idx}.csv\", index=False)\n",
    "\n",
    "    # Save as HTML\n",
    "    with open(output_dir / f\"table_{table_idx}.html\", \"w\") as fp:\n",
    "        fp.write(table.export_to_html(doc=table_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tCfJxhHaIRR"
   },
   "source": [
    "## Image and Figure Extraction\n",
    "\n",
    "Docling also provides image extraction capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EANW0BKX0Q25"
   },
   "source": [
    "### Extracting and Visualizing Page Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNDC06dGaH5M"
   },
   "outputs": [],
   "source": [
    "# Configure pipeline to extract images\n",
    "IMAGE_RESOLUTION_SCALE = 2.0  # 2x resolution (144 DPI)\n",
    "\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    images_scale=IMAGE_RESOLUTION_SCALE,\n",
    "    generate_page_images=True,\n",
    "    generate_picture_images=True,\n",
    ")\n",
    "\n",
    "converter_with_images = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert with image extraction\n",
    "img_result = converter_with_images.convert(docling_paper)\n",
    "img_doc = img_result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create images subdirectory\n",
    "images_dir = output_dir / \"images\"\n",
    "images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save page images\n",
    "for page_no, page in img_doc.pages.items():\n",
    "    page_image_filename = images_dir / f\"page_{page_no}.png\"\n",
    "    with page_image_filename.open(\"wb\") as fp:\n",
    "        page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "print(f\"Saved {len(img_doc.pages)} page images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmfwcH_uUKkp"
   },
   "source": [
    "### Extract Figures and Tables as Images\n",
    "\n",
    "For custom processing, we can also extract figures and tables as images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmxxCUVHUKTl"
   },
   "outputs": [],
   "source": [
    "# Extract all figures and tables as separate images\n",
    "table_counter = 0\n",
    "picture_counter = 0\n",
    "\n",
    "for element, level in img_doc.iterate_items():\n",
    "    if isinstance(element, TableItem):\n",
    "        table_counter += 1\n",
    "        image_filename = images_dir / f\"table_{table_counter}.png\"\n",
    "        with image_filename.open(\"wb\") as fp:\n",
    "            element.get_image(img_doc).save(fp, \"PNG\")\n",
    "\n",
    "    elif isinstance(element, PictureItem):\n",
    "        picture_counter += 1\n",
    "        image_filename = images_dir / f\"figure_{picture_counter}.png\"\n",
    "        with image_filename.open(\"wb\") as fp:\n",
    "            element.get_image(img_doc).save(fp, \"PNG\")\n",
    "\n",
    "print(f\"Extracted {table_counter} tables and {picture_counter} figures as images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pxfi8azdLTQ"
   },
   "source": [
    "### Inspecting Picture Content\n",
    "\n",
    "Docling will automatically generate captions and extract text content from extracted images. Let's take a look at what is extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia--zHY9dNQc"
   },
   "outputs": [],
   "source": [
    "def inspect_pictures_with_images(doc: DoclingDocument, image_size=(6, 4)):\n",
    "    \"\"\"Display pictures inline with their text content\"\"\"\n",
    "    for idx, picture in enumerate(doc.pictures):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Picture {idx}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Display the image\n",
    "        try:\n",
    "            img = picture.get_image(doc)\n",
    "            if img:\n",
    "                plt.figure(figsize=image_size)\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Picture {idx}\")\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display image: {e}\")\n",
    "\n",
    "        # Display metadata\n",
    "        caption = picture.caption_text(doc)\n",
    "        if caption:\n",
    "            print(f\"\\nCaption: {caption}\")\n",
    "\n",
    "        if hasattr(picture, 'prov') and picture.prov:\n",
    "            print(f\"Location: Page {picture.prov[0].page_no}\")\n",
    "\n",
    "        # Display embedded text\n",
    "        print(\"\\nEmbedded text elements:\")\n",
    "        text_found = False\n",
    "        for item, level in doc.iterate_items(root=picture, traverse_pictures=True):\n",
    "            if isinstance(item, TextItem):\n",
    "                print(f\"{'  ' * (level + 1)}- {item.text}\")\n",
    "                text_found = True\n",
    "\n",
    "        if not text_found:\n",
    "            print(\"  (No text elements found)\")\n",
    "\n",
    "# Use the simple inline display\n",
    "inspect_pictures_with_images(img_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hjtRz-mq1Id"
   },
   "source": [
    "### Visualizing Document Layout with Bounding Boxes\n",
    "\n",
    "In order to understand how each part of the document is extracted, let's visualize the extracted elements.\n",
    "\n",
    "We can do that by using one of Docling's built-in visualizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.transforms.visualizer.layout_visualizer import LayoutVisualizer\n",
    "\n",
    "layout_visualizer = LayoutVisualizer()\n",
    "page_images = layout_visualizer.get_visualization(doc=img_doc)\n",
    "\n",
    "num_pages_to_viz = 2  # first N pages to visualize\n",
    "pages_to_viz = list(page_images.keys())[:num_pages_to_viz]\n",
    "for page in pages_to_viz:\n",
    "    display(page_images[page])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkBT1FFUwzYN"
   },
   "source": [
    "## Advanced Features: Enrichment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using vision-language models we have multiple options\n",
    "\n",
    "1. Docling runs the models locally\n",
    "1. Call a local inference server like Ollama and LM Studio\n",
    "1. Use a remote provider offering an openai-compatible api. For the workshop we have the possibility to use a small credit on Replicate. In order to convert the Replicate API to openai-compatible we will launch the LiteLLM proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "RUN_LOCAL_VLM = not IN_COLAB and True\n",
    "RUN_LOCAL_OLLAMA = not IN_COLAB and True\n",
    "# RUN_REMOTE_REPLICATE = IN_COLAB or True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDM-OeUgy0ym"
   },
   "source": [
    "### Picture Classification and Description\n",
    "\n",
    "Beyond basic captioning capabilities, Docling is also able to process images using multimodal LLM's. This will give us more indepth picture descriptions to help us leverage image data more effectively.\n",
    "\n",
    "Let's set up our Display function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.types.doc.document import PictureDescriptionData\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_enriched_document(enr_doc: DoclingDocument):\n",
    "    html_buffer = []\n",
    "    # display the first 5 pictures and their captions and annotations:\n",
    "    for pic in enr_doc.pictures[:5]:\n",
    "        html_item = (\n",
    "            f\"<h3>Picture <code>{pic.self_ref}</code></h3>\"\n",
    "            f'<img src=\"{pic.image.uri!s}\" /><br />'\n",
    "            f\"<h4>Caption</h4>{pic.caption_text(doc=doc)}<br />\"\n",
    "        )\n",
    "        for annotation in pic.annotations:\n",
    "            if not isinstance(annotation, PictureDescriptionData):\n",
    "                continue\n",
    "            html_item += (\n",
    "                f\"<h4>Annotations ({annotation.provenance})</h4>{annotation.text}<br />\\n\"\n",
    "            )\n",
    "        html_buffer.append(html_item)\n",
    "    display(HTML(\"<hr />\".join(html_buffer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And try this with the integrated granite-vision model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXMHPBTdDkev"
   },
   "outputs": [],
   "source": [
    "from docling.datamodel.pipeline_options import PictureDescriptionVlmOptions\n",
    "\n",
    "# Configure enrichment pipeline\n",
    "if RUN_LOCAL_VLM:\n",
    "    # Configure enrichment pipeline\n",
    "    enrichment_options = PdfPipelineOptions(\n",
    "        do_picture_description=True,\n",
    "        picture_description_options=PictureDescriptionVlmOptions(  # model & prompt choice\n",
    "            repo_id=\"ibm-granite/granite-vision-3.2-2b\",\n",
    "            prompt=\"Give a detailed description of what is depicted in the image\",\n",
    "        ),\n",
    "        generate_picture_images=True,\n",
    "        images_scale=2.0,\n",
    "    )\n",
    "\n",
    "    converter_enriched = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=enrichment_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    enr_result = converter_enriched.convert(docling_paper)\n",
    "    enr_doc = enr_result.document\n",
    "\n",
    "    display_enriched_document(enr_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run it using an OpenAI-compatible API like Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.pipeline_options import PictureDescriptionApiOptions\n",
    "\n",
    "if RUN_LOCAL_OLLAMA:\n",
    "    # Configure enrichment pipeline\n",
    "    enrichment_options = PdfPipelineOptions(\n",
    "        do_picture_description=True,\n",
    "        enable_remote_services = True,\n",
    "        picture_description_options=PictureDescriptionApiOptions(\n",
    "            url=\"http://localhost:11434/v1/chat/completions\",\n",
    "            params={\n",
    "                \"model\": \"granite3.2-vision:2b\",\n",
    "                \"max_completion_tokens\": 200,\n",
    "            },\n",
    "            prompt=\"Give a detailed description of what is depicted in the image\",\n",
    "            timeout=60,\n",
    "        ),\n",
    "        generate_picture_images=True,\n",
    "        images_scale=1.0,\n",
    "    )\n",
    "\n",
    "    converter_enriched = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=enrichment_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    enr_result = converter_enriched.convert(docling_paper)\n",
    "    enr_doc = enr_result.document\n",
    "\n",
    "    display_enriched_document(enr_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae4hX3N30sHd"
   },
   "source": [
    "## Visual Language Models (VLMs)\n",
    "\n",
    "A recent feature of Docling is the use of a single shot VLM to process documents. This means that instead the traditional conversion pipeline, we send the docment to a custom tuned VLM that can extract all the content directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1t3ou824ngR"
   },
   "source": [
    "### Using SmolDocling\n",
    "\n",
    "[SmolDocling](https://huggingface.co/ds4sd/SmolDocling-256M-preview) is a compact (256M parameters) vision-language model for document conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE_0sWVl06hs"
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "from docling.datamodel.pipeline_options import (  # Additional imports for VLM\n",
    "    VlmPipelineOptions,\n",
    "    smoldocling_vlm_conversion_options,\n",
    "    smoldocling_vlm_mlx_conversion_options,\n",
    ")\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "if (\n",
    "    \"darwin\" in platform.system().lower()\n",
    "    and \"arm64\" in platform.machine().lower()\n",
    "):  # optimized for Apple Silicon (MLX)\n",
    "    vlm_options = smoldocling_vlm_mlx_conversion_options\n",
    "else:\n",
    "    vlm_options = smoldocling_vlm_conversion_options\n",
    "\n",
    "vlm_pipeline_options = VlmPipelineOptions(\n",
    "    force_backend_text=False,\n",
    "    vlm_options=vlm_options,\n",
    ")\n",
    "\n",
    "converter_vlm = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=vlm_pipeline_options,\n",
    "            pipeline_cls=VlmPipeline,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "vlm_result = converter_vlm.convert(docling_paper)\n",
    "vlm_doc = vlm_result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_md_out = vlm_doc.export_to_markdown()\n",
    "\n",
    "# printing an excerpt\n",
    "print(f\"{vlm_md_out[:2000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UStdCE9f1RgJ"
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What You've Accomplished in Lab 1\n",
    "\n",
    "Congratulations! You've mastered the critical first step in document AI:\n",
    "\n",
    "- Basic and advanced document conversion with visual feedback\n",
    "- Multiple export formats with visualization options\n",
    "- Table and image extraction with visual verification\n",
    "- Enrichment models and VLMs\n",
    "\n",
    "### Your Journey Continues\n",
    "\n",
    "You're now ready for:\n",
    "- **Lab 2**: Learn intelligent chunking strategies to optimize for retrieval\n",
    "- **Lab 3**: Build a complete multimodal RAG system with visual grounding\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice with Your Documents**\n",
    "   - Try different document types\n",
    "   - Experiment with configuration options\n",
    "   - Validate extraction quality\n",
    "\n",
    "2. **Proceed Lab 2**\n",
    "   - Gather a collection of documents\n",
    "   - Think about how you'd want to chunk them\n",
    "   - Consider what makes a good chunk for retrieval\n",
    "\n",
    "Docling provides a powerful, flexible framework for document processing that can be adapted to many use cases. The modular architecture allows you to enable only the features you need, while the visualization capabilities ensure transparency and accuracy in your document processing pipeline.\n",
    "\n",
    "For more information:\n",
    "- GitHub: https://github.com/docling-project/docling\n",
    "- Documentation: https://docling-project.github.io/docling/\n",
    "- Technical Report: https://arxiv.org/abs/2408.09869\n",
    "- Examples: https://github.com/docling-project/docling/tree/main/examples"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
